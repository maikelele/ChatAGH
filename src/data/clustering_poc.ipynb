{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "paths = [\n",
    "    Path(\"/Users/wnowogorski/PycharmProjects/CHAT_AGH/src/data/agh_edu_s1_filtered/KEEP\"),\n",
    "    Path(\"/Users/wnowogorski/PycharmProjects/CHAT_AGH/src/data/agh_edu_s2_filtered/KEEP\"),\n",
    "    Path(\"/Users/wnowogorski/PycharmProjects/CHAT_AGH/src/data/agh_edu_s3_filtered/KEEP\"),\n",
    "    Path(\"/Users/wnowogorski/PycharmProjects/CHAT_AGH/src/data/agh_edu_s4_filtered/KEEP\"),\n",
    "    Path(\"/Users/wnowogorski/PycharmProjects/CHAT_AGH/src/data/agh_edu_s5_filtered/KEEP\"),\n",
    "    Path(\"/Users/wnowogorski/PycharmProjects/CHAT_AGH/src/data/agh_edu_s6_filtered/KEEP\"),\n",
    "    Path(\"/Users/wnowogorski/PycharmProjects/CHAT_AGH/src/data/agh_edu_s7_filtered/KEEP\")\n",
    "]\n",
    "import os\n",
    "import json\n",
    "\n",
    "counter = Counter()\n",
    "\n",
    "files = []\n",
    "for path in paths:\n",
    "    for file in os.listdir(path):\n",
    "        with open(path / file, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        files.append(data)"
   ],
   "id": "c0bf66c21f57f8ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_domain(file):\n",
    "    url = file[\"metadata\"].get(\"url\", \"Not Found\")\n",
    "    if url == \"Not Found\":\n",
    "        return url\n",
    "    else:\n",
    "        domain = url.split(\"/\")[2]\n",
    "        if domain.startswith(\"www.\"):\n",
    "            domain = domain[4:]\n",
    "        return domain\n",
    "\n",
    "Counter([get_domain(f) for f in filtered_files])"
   ],
   "id": "749992a15923c936"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "rejected_domains = [\n",
    "    \"rekrutacja.agh.edu.pl\",\n",
    "    \"dss.agh.edu.pl\",\n",
    "    \"sylabusy.agh.edu.pl\",\n",
    "    \"miasteczko.agh.edu.pl\",\n",
    "    \"akademik.agh.edu.pl\",\n",
    "    \"dss.agh.edu.pl\",\n",
    "    \"sylabusy.agh.edu.pl\",\n",
    "    \"historia_agh.agh.edu.pl\"\n",
    "]\n",
    "\n",
    "filtered_files = [file for file in files if get_domain(file) not in rejected_domains]"
   ],
   "id": "5f8afc2ba169b8f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "history_agh = [file for file in files if get_domain(file) == \"historia_agh.agh.edu.pl\"]\n",
    "\n",
    "for i, file in enumerate(history_agh):\n",
    "    with open(f\"/collections/historia_agh/{i}.json\", \"w\") as f:\n",
    "        json.dump(file, f)"
   ],
   "id": "5a6d3a5baf072d2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for i, file in enumerate(filtered_files[:1100]):\n",
    "    with open(f\"/Users/wnowogorski/PycharmProjects/CHAT_AGH/src/collections/agh_edu_1/{i}.json\", \"w\") as f:\n",
    "        json.dump(file, f)\n",
    "\n",
    "for i, file in enumerate(filtered_files[1100:]):\n",
    "    with open(f\"/Users/wnowogorski/PycharmProjects/CHAT_AGH/src/collections/agh_edu_2/{i}.json\", \"w\") as f:\n",
    "        json.dump(file, f)"
   ],
   "id": "399477aad1f14722"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# for i, file in enumerate(filtered_files[:1200]):\n",
    "#     with open(f\"/Users/wnowogorski/PycharmProjects/CHAT_AGH/src/collections/agh_edu1/{i}.json\", \"w\") as f:\n",
    "#         json.dump(file, f)\n",
    "\n",
    "for i, file in enumerate(filtered_files[:500]):\n",
    "    with open(f\"/Users/wnowogorski/PycharmProjects/CHAT_AGH/src/collections/agh_edu_1/{i}.json\", \"w\") as f:\n",
    "        json.dump(file, f)\n",
    "\n",
    "for i, file in enumerate(filtered_files[500:1000]):\n",
    "    with open(f\"/Users/wnowogorski/PycharmProjects/CHAT_AGH/src/collections/agh_edu_2/{i}.json\", \"w\") as f:\n",
    "        json.dump(file, f)\n",
    "\n",
    "for i, file in enumerate(filtered_files[1000:1500]):\n",
    "    with open(f\"/Users/wnowogorski/PycharmProjects/CHAT_AGH/src/collections/agh_edu_3/{i}.json\", \"w\") as f:\n",
    "        json.dump(file, f)\n",
    "\n",
    "for i, file in enumerate(filtered_files[1500:]):\n",
    "    with open(f\"/Users/wnowogorski/PycharmProjects/CHAT_AGH/src/collections/agh_edu_4/{i}.json\", \"w\") as f:\n",
    "        json.dump(file, f)"
   ],
   "id": "6bda047a7e9274e7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "path = \"/Users/wnowogorski/PycharmProjects/CHAT_AGH/src/collections/sylabusy_agh/\"\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    if file.endswith(\".md\"):\n",
    "        with open(path + file, \"r\") as f:\n",
    "            content = f.read()\n",
    "\n",
    "        metadata_name = file.split(\".\")[0]\n",
    "        with open(path + metadata_name + \"_meta.json\", \"r\") as f:\n",
    "            metadata = json.load(f)\n",
    "\n",
    "        with open(f\"{metadata_name}.json\", \"w\") as f:\n",
    "            json.dump({\n",
    "                \"content\": content,\n",
    "                \"metadata\": metadata,\n",
    "            }, f)"
   ],
   "id": "42254e2f3ce8f56e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Extract domains and prepare data for clustering\n",
    "processed_data = []\n",
    "\n",
    "for item in tqdm(filtered_files):\n",
    "    # Extract domain\n",
    "    domain = get_domain(item)\n",
    "\n",
    "    # Get content\n",
    "    content = item.get(\"content\", \"\")\n",
    "\n",
    "    # Only include items that have content\n",
    "    if content and len(content.strip()) > 0:\n",
    "        processed_data.append({\n",
    "            \"domain\": domain,\n",
    "            \"content\": content,\n",
    "            \"metadata\": item.get(\"metadata\", {})\n",
    "        })\n",
    "\n",
    "print(f\"Processed {len(processed_data)} documents with content\")\n",
    "\n",
    "# Create a DataFrame for easier manipulation\n",
    "df = pd.DataFrame(processed_data)\n",
    "\n",
    "# Check domain distribution\n",
    "domain_counts = df['domain'].value_counts()\n",
    "print(f\"Found {len(domain_counts)} unique domains\")\n",
    "domain_counts.head(10)"
   ],
   "id": "ec116a6e44d499b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create embeddings using SentenceTransformers\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Using a smaller model for speed, you can use 'all-mpnet-base-v2' for better quality\n",
    "\n",
    "# Generate embeddings (this might take some time for large datasets)\n",
    "print(\"Generating embeddings...\")\n",
    "embeddings = []\n",
    "\n",
    "# Process in batches to avoid memory issues\n",
    "batch_size = 32\n",
    "for i in tqdm(range(0, len(df), batch_size)):\n",
    "    batch = df['content'].iloc[i:i+batch_size].tolist()\n",
    "    batch_embeddings = model.encode(batch)\n",
    "    embeddings.extend(batch_embeddings)\n",
    "\n",
    "embeddings = np.array(embeddings)\n",
    "print(f\"Generated embeddings with shape: {embeddings.shape}\")"
   ],
   "id": "8f05892cb3580c20"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Determine optimal number of clusters using the elbow method\n",
    "def plot_elbow(embeddings, max_k=15):\n",
    "    inertias = []\n",
    "    for k in range(1, max_k+1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        kmeans.fit(embeddings)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, max_k+1), inertias, marker='o')\n",
    "    plt.title('Elbow Method for Optimal k')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Inertia')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Run elbow method to find optimal k\n",
    "plot_elbow(embeddings)\n",
    "\n",
    "# Set the number of clusters (adjust based on elbow plot)\n",
    "n_clusters = 10  # Change this based on the elbow plot\n",
    "\n",
    "# Run KMeans clustering\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(embeddings)\n",
    "\n",
    "# Add cluster assignments to DataFrame\n",
    "df['cluster'] = clusters"
   ],
   "id": "989782e658f44a08"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Visualize clusters using PCA for dimensionality reduction\n",
    "pca = PCA(n_components=2)\n",
    "reduced_embeddings = pca.fit_transform(embeddings)\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "viz_df = pd.DataFrame({\n",
    "    'x': reduced_embeddings[:, 0],\n",
    "    'y': reduced_embeddings[:, 1],\n",
    "    'cluster': clusters,\n",
    "    'domain': df['domain'].values\n",
    "})\n",
    "\n",
    "# Plot the clusters\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(x='x', y='y', hue='cluster', data=viz_df, palette='viridis', s=50, alpha=0.7)\n",
    "plt.title('Document Clusters Visualization')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.legend(title='Cluster')\n",
    "plt.show()"
   ],
   "id": "8187d9fa76f529a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Function to get top words for each cluster\n",
    "def get_cluster_keywords(df, cluster_id, top_n=10):\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "    # Get documents in the cluster\n",
    "    cluster_docs = df[df['cluster'] == cluster_id]['content'].tolist()\n",
    "\n",
    "    # Create TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer(max_features=100, stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(cluster_docs)\n",
    "\n",
    "    # Get top words based on TF-IDF scores\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    tfidf_scores = tfidf_matrix.sum(axis=0).A1\n",
    "\n",
    "    # Sort words by TF-IDF scores\n",
    "    top_words_idx = tfidf_scores.argsort()[-top_n:][::-1]\n",
    "    top_words = [feature_names[i] for i in top_words_idx]\n",
    "\n",
    "    return top_words\n",
    "\n",
    "# Analyze each cluster\n",
    "for cluster_id in range(n_clusters):\n",
    "    cluster_size = len(df[df['cluster'] == cluster_id])\n",
    "    print(f\"\\nCluster {cluster_id} - Size: {cluster_size} documents ({cluster_size/len(df)*100:.2f}%)\")\n",
    "\n",
    "    # Top keywords\n",
    "    keywords = get_cluster_keywords(df, cluster_id)\n",
    "    print(f\"Top keywords: {', '.join(keywords)}\")\n",
    "\n",
    "    # Domain distribution within cluster\n",
    "    cluster_domains = df[df['cluster'] == cluster_id]['domain'].value_counts()\n",
    "    print(f\"Number of unique domains in cluster: {len(cluster_domains)}\")\n",
    "    print(\"Top domains in this cluster:\")\n",
    "    for domain, count in cluster_domains.head(5).items():\n",
    "        print(f\"  - {domain}: {count} documents ({count/cluster_size*100:.2f}% of cluster)\")"
   ],
   "id": "58d9a424b75e6185"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create a domain-cluster matrix - FIXED to ensure numeric data types\n",
    "def analyze_domain_cluster_distribution(df, n_clusters):\n",
    "    # Get domains that appear in at least 5 documents\n",
    "    common_domains = df['domain'].value_counts()[df['domain'].value_counts() >= 5].index.tolist()\n",
    "\n",
    "    # Create a domain-cluster matrix with explicit numeric dtype\n",
    "    domain_cluster_matrix = pd.DataFrame(index=common_domains, columns=range(n_clusters), dtype=float)\n",
    "\n",
    "    # Initialize with zeros to ensure numeric type\n",
    "    for column in range(n_clusters):\n",
    "        domain_cluster_matrix[column] = 0.0\n",
    "\n",
    "    for domain in common_domains:\n",
    "        domain_docs = df[df['domain'] == domain]\n",
    "        domain_clusters = domain_docs['cluster'].value_counts()\n",
    "\n",
    "        for cluster in range(n_clusters):\n",
    "            # Calculate percentage of domain's documents in each cluster\n",
    "            count = domain_clusters.get(cluster, 0)\n",
    "            if len(domain_docs) > 0:  # Avoid division by zero\n",
    "                domain_cluster_matrix.loc[domain, cluster] = float(count / len(domain_docs) * 100)\n",
    "\n",
    "    # Ensure all data is numeric type\n",
    "    domain_cluster_matrix = domain_cluster_matrix.astype(float)\n",
    "    return domain_cluster_matrix\n",
    "\n",
    "# Generate and display the matrix\n",
    "domain_cluster_matrix = analyze_domain_cluster_distribution(df, n_clusters)\n",
    "\n",
    "# Check to make sure we have numeric data\n",
    "print(\"Matrix data type:\", domain_cluster_matrix.dtypes[0])\n",
    "\n",
    "# Visualize the matrix as a heatmap\n",
    "plt.figure(figsize=(12, max(8, len(domain_cluster_matrix) * 0.4)))\n",
    "sns.heatmap(domain_cluster_matrix, cmap='YlGnBu', annot=True, fmt='.1f')\n",
    "plt.title('Domain Distribution Across Clusters (%)')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Domain')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "dce452edb46d1bc9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create an alternative visualization of domain-cluster distribution\n",
    "# This approach is more robust against type issues\n",
    "\n",
    "# For each domain with at least 5 documents, show distribution across clusters\n",
    "top_domains = df['domain'].value_counts()[df['domain'].value_counts() >= 5].index.tolist()[:15]  # Limit to top 15\n",
    "\n",
    "# Create DataFrame for plotting\n",
    "plot_data = []\n",
    "for domain in top_domains:\n",
    "    domain_docs = df[df['domain'] == domain]\n",
    "    total_docs = len(domain_docs)\n",
    "    for cluster in range(n_clusters):\n",
    "        cluster_docs = domain_docs[domain_docs['cluster'] == cluster]\n",
    "        percentage = len(cluster_docs) / total_docs * 100 if total_docs > 0 else 0\n",
    "        plot_data.append({\n",
    "            'Domain': domain,\n",
    "            'Cluster': f'Cluster {cluster}',\n",
    "            'Percentage': percentage\n",
    "        })\n",
    "\n",
    "plot_df = pd.DataFrame(plot_data)\n",
    "\n",
    "# Plot the distribution\n",
    "plt.figure(figsize=(14, 10))\n",
    "domain_cluster_plot = sns.barplot(x='Domain', y='Percentage', hue='Cluster', data=plot_df)\n",
    "plt.title('Domain Distribution Across Clusters')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel('Percentage of Domain Documents (%)')\n",
    "plt.legend(title='Cluster')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "b95c040d27a1038d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Calculate Silhouette Score to evaluate clustering quality\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "\n",
    "silhouette_avg = silhouette_score(embeddings, clusters)\n",
    "calinski_harabasz_avg = calinski_harabasz_score(embeddings, clusters)\n",
    "\n",
    "print(f\"Silhouette Score: {silhouette_avg:.4f}\")\n",
    "print(f\"Calinski-Harabasz Score: {calinski_harabasz_avg:.4f}\")\n",
    "\n",
    "# Higher silhouette scores (closer to 1) indicate better-defined clusters\n",
    "# Higher Calinski-Harabasz scores indicate better cluster separation"
   ],
   "id": "15bce82ea51a9c25"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Find domains that are predominantly in one cluster (cluster specificity)\n",
    "def analyze_domain_specificity(df, n_clusters):\n",
    "    # Get domains with at least 5 documents\n",
    "    domains = df['domain'].value_counts()[df['domain'].value_counts() >= 5].index\n",
    "\n",
    "    domain_specificity = []\n",
    "    for domain in domains:\n",
    "        domain_docs = df[df['domain'] == domain]\n",
    "        total_docs = len(domain_docs)\n",
    "\n",
    "        # Get distribution across clusters\n",
    "        cluster_distribution = domain_docs['cluster'].value_counts(normalize=True) * 100\n",
    "\n",
    "        # Find dominant cluster\n",
    "        dominant_cluster = cluster_distribution.idxmax()\n",
    "        dominant_percentage = cluster_distribution.max()\n",
    "\n",
    "        domain_specificity.append({\n",
    "            'domain': domain,\n",
    "            'dominant_cluster': dominant_cluster,\n",
    "            'dominant_percentage': dominant_percentage,\n",
    "            'total_docs': total_docs\n",
    "        })\n",
    "\n",
    "    # Convert to DataFrame and sort by specificity\n",
    "    specificity_df = pd.DataFrame(domain_specificity)\n",
    "    return specificity_df.sort_values('dominant_percentage', ascending=False)\n",
    "\n",
    "# Get domain specificity\n",
    "domain_specificity = analyze_domain_specificity(df, n_clusters)\n",
    "\n",
    "# Show the most cluster-specific domains\n",
    "print(\"Domains most strongly associated with a single cluster:\")\n",
    "display(domain_specificity.head(15))\n",
    "\n",
    "# Find the most representative documents for each cluster\n",
    "def get_representative_docs(df, kmeans, embeddings, n=3):\n",
    "    \"\"\"Get the documents closest to each cluster centroid\"\"\"\n",
    "    centers = kmeans.cluster_centers_\n",
    "\n",
    "    representative_docs = []\n",
    "\n",
    "    for i in range(len(centers)):\n",
    "        # Calculate distance from each document to the cluster center\n",
    "        distances = np.linalg.norm(embeddings - centers[i], axis=1)\n",
    "\n",
    "        # Get indices of documents closest to the center\n",
    "        closest_indices = np.argsort(distances)[:n]\n",
    "\n",
    "        # Get those documents\n",
    "        cluster_docs = df.iloc[closest_indices][['domain', 'content']]\n",
    "\n",
    "        for _, doc in cluster_docs.iterrows():\n",
    "            # Truncate content for display\n",
    "            content = doc['content']\n",
    "            if len(content) > 300:\n",
    "                content = content[:300] + \"...\"\n",
    "\n",
    "            representative_docs.append({\n",
    "                'cluster': i,\n",
    "                'domain': doc['domain'],\n",
    "                'content': content\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(representative_docs)\n",
    "\n",
    "# Get representative documents\n",
    "rep_docs = get_representative_docs(df, kmeans, embeddings)\n",
    "\n",
    "# Display representative documents by cluster\n",
    "for cluster in range(n_clusters):\n",
    "    print(f\"\\n=== CLUSTER {cluster} REPRESENTATIVE DOCUMENTS ===\")\n",
    "    cluster_docs = rep_docs[rep_docs['cluster'] == cluster]\n",
    "    for i, (_, doc) in enumerate(cluster_docs.iterrows()):\n",
    "        print(f\"\\nDocument {i+1} from {doc['domain']}:\")\n",
    "        print(doc['content'])\n",
    "        print(\"-\" * 80)"
   ],
   "id": "ed221f2d1fc814f0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9cded3cb0e6fdaa9"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
